{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d378dbf",
   "metadata": {},
   "source": [
    "# LSTM Time-Series Forecasting for SPY \n",
    "**Project:** Time-series forecasting for equities using LSTM\n",
    "\n",
    "**Author:** Khalid Hajeer\n",
    "\n",
    "**Last updated:** September 13, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e04c0b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Setup & Reproducibility](#2-setup--reproducibility)  \n",
    "3. [Data Collection & Preprocessing](#3-data-collection--preprocessing)  \n",
    "4. [Exploratory Data Analysis (EDA)](#4-exploratory-data-analysis-eda)  \n",
    "5. [Feature Engineering](#5-feature-engineering)\n",
    "6. [Systematic Feature Selection](#6-systematic-feature-selection)  \n",
    "7. [Baselines](#7-baselines)  \n",
    "8. [LSTM Data Preparation & Architecture](#8-lstm-data-preparation--architecture)  \n",
    "9. [Training & Validation](#9-training--validation) \n",
    "10. [Test Evaluation & Backtest](#10-test-evaluation--backtest)\n",
    "11. [Conclusion](#11-conclusion)\n",
    "12. [Appendex](#12-Appendex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b00b9",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65215a4",
   "metadata": {},
   "source": [
    "The **SPDR S&P 500 ETF Trust (SPY)** tracks the performance of the S&P500 index - a benchmark of 500 leading publicly listed companies in the United States. SPY is highly liquid and often used as a proxy for the overall U.S. stock market.\n",
    "\n",
    "> **Note:** Long run performance figures vary by window and methodology. In this project, SPY serves primarily as a liquid benchmark for modelling and backtesting rather than as a statement about historical returns.\n",
    "\n",
    "Finance time-series forecasting is challenging: signals are weak, noisy, and non-stationary. Patterns can emerge, decay, or invert with regime shifts and market feedback. We therefore use strict chronological splits, early stopping on validation correlation, refitting before test, and cost-aware backtesting to assess whether any learned structure is robust.\n",
    "\n",
    "### Objectives\n",
    "Build and evaluate a **Long Short-Term Memory (LSTM)** network (a type of Recurrent Neural Network (RNN)) to predict the **next day return** of SPY. We model **log returns** instead of raw prices for better stationarity and comparability across time.\n",
    "$$\n",
    "r_t = \\ln \\left(\\frac{C_t}{C_{t-1}}\\right) = \\ln C_t - \\ln C_{t-1}.\n",
    "$$\n",
    "where:\n",
    "- $r_t$ is the log return at day $t$\n",
    "- $C_t$ is close at day $t$\n",
    "- $C_{t-1}$ is close at day $t-1$\n",
    "\n",
    "From a predicted log return $\\hat{r}_{t+1}$, we recover the next day close:\n",
    "$$\n",
    "\\hat{C}_{t+1} = C_t \\cdot e^{\\hat{r}_{t+1}}\n",
    "$$\n",
    "\n",
    "### Methodology Overview\n",
    "\n",
    "**Setup & Reproducibility**  \n",
    "    Import libraries; set deterministic seeds; define a central config (paths, splits, dates, costs).\n",
    "\n",
    "**Data Collection & Preprocessing**  \n",
    "    Pull daily OHLCV (Open, High, Low, Close, Volume) from Alpaca; validate monotonic timestamps; enforce dtypes; handle missing values; verify no lookahead in any merge; persist a clean, versioned dataset.\n",
    "\n",
    "**Exploratory Data Analysis (EDA)**  \n",
    "    Plot distributions and volatility; examine regime hints; inspect missingness and outliers.\n",
    "\n",
    "**Feature Engineering**   \n",
    "    Build lagged/rolling features (momentum, volatility, volume, statistical); shift all predictors to t-1 to avoid lookahead; persist a versioned feature catalog.\n",
    "\n",
    "**Systematic Feature Selection**   \n",
    "    Rank features on train only; freeze the selected set for modelling.\n",
    "\n",
    "**Baselines**  \n",
    "    Train Ridge and XGBoost baselines plus a naïve persistence/momentum rule; evaluate with identical splits and metrics (Pearson correlation, MSE, and trading PnL metrics).\n",
    "\n",
    "**LSTM Data Preparation & Architecture**   \n",
    "    Fit scalers on train only; create a sliding window sequence; specify LSTM layers, units, dropout with fixed seeds; checkpoint models; ensure target alignment.\n",
    "\n",
    "**Training, Validation & Model Selection**  \n",
    "    Train with early stop on validation Pearson correlation; record the best epoch along with its weights and metrics. The best epoch is then used to refit the model from scratch on combined train+val set (without early_stopping), after which test predictions are generated and all artefacts are archived.\n",
    "\n",
    "**Backtest & Test Evaluation**  \n",
    "    Map signal -> position; compare against baselines and a label-shuffle control; report Sharpe, MaxDD, Turnover; plot a $1k wealth chart for the test set.\n",
    "\n",
    "**Diagnostics**  \n",
    "    Inspect rolling correlation/Sharpe, error-by-volatility profiles, and residual ACF; review feature usage/coverage and data leakage checks.\n",
    "\n",
    "**Conclusions**  \n",
    "    Summarise findings; discuss limitations; suggest next steps.\n",
    "\n",
    "### Why LSTM?\n",
    "LSTMs capture patterns in sequential and temporal data by learning which parts of past information are most relevant for future predictions. They are a type of Recurring Neural Network (RNN) designed to overcome the vanishing gradient problem, where gradients become extremely small during backpropagation through time, making it difficult for standard RNNs to learn long-term dependencies.\n",
    "By incorporating gating mechanisms, LSTMs can retain important information over longer sequences, making them particularly suited for stock market time-series forecasting.\n",
    "\n",
    "### Scope & Assumptions\n",
    "- The approach generalises to any Alpaca-supported ticker; performance will vary by asset, liquidity, and regime.\n",
    "- Corporate actions (splits/dividends) and survivorship biases must be handled explicitly.\n",
    "- Inputs are historical OHLCV + engineered features only (no macro/news)\n",
    "- All transformations are fit on training data only and applied forward to avoid leakage.\n",
    "\n",
    "### Expected Output\n",
    "- A trained LSTM model that predicts next-day log returns.\n",
    "- A test-only backtest report with after-cost metrics, $1k wealth chart, baselines, and a shuffle control.\n",
    "- Visual comparisons: predicted vs. realised returns (and derived prices).\n",
    "- One-step-ahead forecast for the next trading day\n",
    "- A concise tearsheet and a longer Methods PDF\n",
    "\n",
    "### Financial Disclaimer\n",
    "This project is for **educational purposes only** and is **not financial advice**.\n",
    "Model outputs are uncertain and should not be used for live trading without independent verification and risk controls.\n",
    "\n",
    "> \"If you were to give a monkey a typewriter and let him randomly hit keys forever, then he would eventually type Shakespeare's Hamlet - word for word.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004edbd",
   "metadata": {},
   "source": [
    "## 2. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cf72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries \n",
    "import os, sys, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project modules\n",
    "# Add parent directory (so Python can see src/)\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "# TODO: Update requirements.txt at the end\n",
    "from src.data import get_stock_data, align_calendar\n",
    "from src.features import make_targets, build_feature_matrix\n",
    "from src.validate import run_all_validations\n",
    "from src.metrics import *\n",
    "from src.backtest import single_asset_backtest\n",
    "from src.models.baseline import *\n",
    "from src.utils import *\n",
    "\n",
    "# Set seed for reproducibility\n",
    "GLOBAL_SEED = 42\n",
    "random.seed(GLOBAL_SEED)\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "tf.random.set_seed(GLOBAL_SEED)\n",
    "\n",
    "pd.options.display.float_format = \"{:.6f}\".format\n",
    "print(\"Imports ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7bb5d",
   "metadata": {},
   "source": [
    "## 3. Data Collection & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7104b94",
   "metadata": {},
   "source": [
    "First, let's fetch the data we'll be working with.\n",
    "\n",
    "We'll use the Alpaca API credentials from the environment and fetch 10 years of daily OHLCV (+ VWAP, trade_count) data for SPY.\n",
    "\n",
    "To save time on future runs, the dataet is cached locally at 'data/raw/SPY.parquet'.\n",
    "If the cache is missing or we set 'FORCE_REFRESH=True', the data will be pulled fresh from Alpaca and saved to that path.\n",
    "\n",
    "After fetching, we make the data is clean and ready:\n",
    "- enforce a UTC 'DatetimeIndex' that is monotonic and unique,\n",
    "- coerce numeric dtypes, and remove duplicates/missing values,\n",
    "\n",
    "Finally, we align the dataset with the trading calendar, ensuring no gaps remain before downstream feature engineering and backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API credentials from .env file (ALPACA_API_KEY, ALPACA_SECRET_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "symbol = \"SPY\"\n",
    "CACHE_PATH = \"../data/raw/SPY.parquet\" # local cache location\n",
    "FORCE_REFRESH = True # set to True to fetch fresh data\n",
    "\n",
    "# Fetch data from Alpaca if needed, else load from cache\n",
    "# Output is a Pandas DataFrame with UTC DatetimeIndex\n",
    "df = get_stock_data(\n",
    "    symbol=symbol,\n",
    "    years=5,\n",
    "    path=CACHE_PATH,\n",
    "    force_refresh=FORCE_REFRESH,\n",
    "    feed=\"iex\"\n",
    ")\n",
    "\n",
    "# Align calendar: ensuring  sorted, unique UTC DatetimeIndex\n",
    "df = align_calendar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889b303",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea34bf6",
   "metadata": {},
   "source": [
    "Before feature engineering, let's explore the dataset and get a sense of what we're working with.\n",
    "\n",
    "We'll start with a quick tabulat overview:\n",
    "- look at the first few rows to see the raw structure,\n",
    "- check datatypes and missing values,\n",
    "- review summary statistics like mean, std, and ranges\n",
    "\n",
    "After that, we'll plot the closing price and trading value over time to spot trends, volatility clusters, and any unusual behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ffa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows\n",
    "print(\"Head of DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Concise summary: dtypes, non-null counts, memory usage\n",
    "print(\"\\n DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Descriptive statistics: mean, std, min, max, percentiles\n",
    "print(\"\\n DataFrame Descriptive Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the close price over time\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(df.index, df['close'], label='Close Price', color='navy')\n",
    "plt.title('SPY Close Price Over Time', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price ($)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot the volume over time\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(df.index, df['volume'], label='Volume', color='darkorange')\n",
    "plt.title('SPY Trading Volume Over Time')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Shares Traded', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62547a",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253df358",
   "metadata": {},
   "source": [
    "Now let's build the targets and the feature matrix using our module functions.\n",
    "\n",
    "**Why returns (not prices)?**\n",
    "Returns are closer to stationary and live on a comparable scale across time.\n",
    "We'll predict next day log returns and also keep a volatility-normalised target for diagnostics.\n",
    "\n",
    "**What we'll do here**\n",
    "1) Create targets ('y_raw', 'y_vol') from close prices.\n",
    "2) Build a lag-safe feature matrix from our raw data (including price, technical, volume, volatility, and statistical blocks) via 'build_feature_matrix'.\n",
    "3) Run validation checks to guard against common issues (index, NaNs/inf, and obvious lookahead leakage) \n",
    "4) Quick preview + visuals\n",
    "\n",
    "All transforms use past-only information and apply a central lag so that features at timet $t$ predict $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f79e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Targets from close prices\n",
    "targets = make_targets(df[\"close\"], vol_span=20) # -> columns: log_return, ewma_vol_20, y_raw, y_vol\n",
    "\n",
    "# 2) Feature matrix (lag-safe; applies a central lag so X_t predicts y_{t+1})\n",
    "feature_matrix = build_feature_matrix(df, lag_days=1, drop_na_rows=True)\n",
    "\n",
    "# 3) Validations (index, NaNs/inf, basic anti-leakage structure)\n",
    "run_all_validations(\n",
    "    raw=df,\n",
    "    X=feature_matrix,\n",
    "    y=targets[\"y_raw\"].reindex(feature_matrix.index), # align to X\n",
    "    required_raw_cols=(\"open\",\"high\",\"low\",\"close\"),\n",
    "    expected_central_lag_days=1\n",
    ")\n",
    "\n",
    "# 4) Keep a working frame for quick inspection\n",
    "features = feature_matrix.join(targets[[\"y_raw\", \"y_vol\"]], how=\"inner\")\n",
    "print(features.head(10))\n",
    "print(f\"\\nFeature shape: {feature_matrix.shape}, with {feature_matrix.columns.nunique()} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e327a80",
   "metadata": {},
   "source": [
    "### 5.1 What's inside the matrix?\n",
    "\n",
    "- **Price-based:** spreads (high-low, close-ipen), ratios (close/open), simple & log returns, short-horizon momentum.\n",
    "- **Technical:** SMA, EMA, RSI(14), MACD(line + signal + histogram), Bollinger Bands (lower, middle, upper).\n",
    "-  **Volatility & Rolling Stats:** rolling $\\sigma$ of returns, rolling mean/std, z-scores.\n",
    "- **Volume-based:** $\\Delta$ volume, volume/avg, OBV, VWAP deviations, trade count stats.\n",
    "- **Statistical:** Shannon entropy (windows), Hurst exponent (long memory), rolling autocorrelations.\n",
    "\n",
    "All columns are shifted to $t-1$ within the orchestrator so there's no lookahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a99d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"feature matrix info:\")\n",
    "feature_matrix.info()\n",
    "\n",
    "print(\"\\nNaN counts (should be 0 after drop_na_rows=true):\")\n",
    "na_counts = feature_matrix.isna().sum().sort_values(ascending=False).head(10)\n",
    "print(na_counts.to_string())\n",
    "\n",
    "print(\"\\nTargets preview:\")\n",
    "aligned_targets = targets[[\"y_raw\", \"y_vol\"]].reindex(feature_matrix.index)\n",
    "print(aligned_targets.describe().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c68017",
   "metadata": {},
   "source": [
    "### 5.2 Quick Visuals\n",
    "\n",
    "A couple of lightweight checks help spot obvious issues before modelling:\n",
    "- **Indicator subplots** on a recent slice (e.g., last 500 days)\n",
    "- **Correlation heatmap** (high-level view to spot multicollinearity clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator subplots\n",
    "\n",
    "# using last 500 days (to keep plot readable)\n",
    "df_plot = features.tail(500).copy()\n",
    "\n",
    "fig, (ax1,  ax2, ax3) = plt.subplots(3, 1, figsize=(15,12), sharex=True)\n",
    "\n",
    "# Price\n",
    "ax1.plot(df_plot.index, df.loc[df_plot.index, 'close'], label='Close Price', color='navy')\n",
    "ax1.set_title(\"Price with MACD & RSI (last 500 days)\")\n",
    "ax1.set_ylabel(\"Price ($)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# MACD\n",
    "ax2.plot(df_plot.index, feature_matrix.loc[df_plot.index, \"macd\"], label=\"MACD\", color='green')\n",
    "ax2.set_ylabel(\"MACD\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# RSI\n",
    "ax3.plot(df_plot.index, feature_matrix.loc[df_plot.index, \"rsi_14\"], label=\"RSI (14)\", color='darkgreen')\n",
    "ax3.axhline(70, color='red', linestyle='--', alpha=0.5, label='Overbought (70)')\n",
    "ax3.axhline(30, color='red', linestyle='--', alpha=0.5, label='Oversold (30)')\n",
    "ax3.set_ylabel(\"RSI\")\n",
    "ax3.set_xlabel(\"Date\")\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e85547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "FAMILIES = [\n",
    "    \"sma_\", \"ema_\", \"rsi_\", \"macd\", \"bb_\",  # technicals\n",
    "    \"momentum\", \"simple_return\", \"log_return\",            # price-derived\n",
    "    \"vol_\", \"rolling_std_\", \"rolling_mean_\", \"z_score_\",  # volatility/stats\n",
    "    \"obv\", \"volume_\", \"volume_over_avg\", \"trade_count\",   # volume\n",
    "    \"entropy\", \"hurst\", \"autocorr\"                        # statistical\n",
    "]\n",
    "cols_to_show = [c for c in feature_matrix.columns if any(prefix in c for prefix in FAMILIES)][:25]\n",
    "\n",
    "correlation = feature_matrix[cols_to_show].corr().to_numpy()\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(correlation, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.title(\"Feature Correlation Heatmap (subset)\")\n",
    "plt.xticks(range(len(cols_to_show)), cols_to_show, rotation=90)\n",
    "plt.yticks(range(len(cols_to_show)), cols_to_show)\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd482bec",
   "metadata": {},
   "source": [
    "## 6. Systematic Feature Selection\n",
    "We've engineered ~40+ features; some help, some don't. Feeding all of them into our LSTM can mask the real patterns, slow training , and hurt generalisation. Instead, we'll rank features on the training window only and freeze a compact set for modelling.\n",
    "\n",
    "**What we'll do**\n",
    "1) Define train/val/test** splits (chronological).\n",
    "2) Fit a Random Forest Regressor on train only.\n",
    "3) Compute permutation importance on the validation window (model-agnostic reduces Random Forest bias).\n",
    "4) Pick the top-N features and optionally deduplicate highly correlated ones.\n",
    "5) Freeze the selected feature list for downstream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1547d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y from targets, aligned to feature_matrix index\n",
    "y = targets[\"y_raw\"].reindex(feature_matrix.index)\n",
    "\n",
    "# Keep numeric features only; clean infs; align & drop NaNs\n",
    "X = (\n",
    "    feature_matrix\n",
    "    .select_dtypes(include=[np.number])\n",
    "    .replace([np.inf, -np.inf], np.nan)\n",
    ")\n",
    "X, y = align_X_y(X, y, dropna=True)\n",
    "\n",
    "# Chronological split: Train / Val / Test (70, 15, 15)\n",
    "n = len(X)\n",
    "i_train = int(n * 0.70)\n",
    "i_val = int(n * 0.85)\n",
    "\n",
    "X_train, y_train = X.iloc[:i_train], y.iloc[:i_train]\n",
    "X_val, y_val = X.iloc[i_train:i_val], y.iloc[i_train:i_val]\n",
    "X_test, y_test = X.iloc[i_val:], y.iloc[i_val:]\n",
    "\n",
    "print(f\"Shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Fit RF on Train ONLY\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=600,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# Permutation importance on VALIDATION, scored by Pearson correlation\n",
    "r_scorer = make_scorer(pearson_corr, greater_is_better=True)\n",
    "\n",
    "perm = permutation_importance(\n",
    "    rf, X_val, y_val,\n",
    "    n_repeats=20,\n",
    "    random_state=42,\n",
    "    scoring=r_scorer\n",
    ")\n",
    "\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": X_val.columns,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std\n",
    "    })\n",
    "    .sort_values(\"importance_mean\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Select top-N and prune near-duplicates\n",
    "N = 20\n",
    "top_n_features = imp_df[\"feature\"].tolist()\n",
    "\n",
    "def prune_redundant(features, Xref, corr_threshold=0.95):\n",
    "    kept = []\n",
    "    for feature in features:\n",
    "        if not kept:\n",
    "            kept.append(feature); continue\n",
    "        if all (abs(Xref[feature].corr(Xref[k])) < corr_threshold for k in kept):\n",
    "            kept.append(feature)\n",
    "    return kept\n",
    "    \n",
    "SELECTED_FEATURES = prune_redundant(top_n_features, X_train, corr_threshold=0.95)\n",
    "print(f\"\\nTop {N} by permutation importance:\\n {top_n_features}\")\n",
    "print(f\"\\nAfter redundancy pruning ({len(SELECTED_FEATURES)} kept):\\n{SELECTED_FEATURES}\")\n",
    "\n",
    "# Freeze to join JSON for reproducability\n",
    "ARTIFACTS_DIR = Path(\"artifacts\"); ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURE_SET_PATH = ARTIFACTS_DIR / \"feature_set.json\"\n",
    "\n",
    "payload = {\n",
    "    \"selected_features\": SELECTED_FEATURES,\n",
    "    \"meta\": {\n",
    "        \"model\": \"RandomForestRegressor\",\n",
    "        \"n_estimators\": 600,\n",
    "        \"scoring\": \"pearson_r\",\n",
    "        \"n_repeats\": 20,\n",
    "        \"prune_threshold\": 0.95,\n",
    "        \"train_range\": [str(X_train.index.min()), str(X_train.index.max())],\n",
    "        \"val_range\":   [str(X_val.index.min()),   str(X_val.index.max())],\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "}\n",
    "FEATURE_SET_PATH.write_text(json.dumps(payload, indent=2))\n",
    "print(f\"\\nSaved frozen feature set → {FEATURE_SET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0564c",
   "metadata": {},
   "source": [
    "## §7. Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b94d4",
   "metadata": {},
   "source": [
    "Before we build the LSTM, let's set simple, transparent baselines to sanity check signal quality and avoid overfitting. These serve as benchmarks that any more sophisticated model should surpass.\n",
    "\n",
    "What we evaluate\n",
    "- **Persistence ($r_t -> r_{t+1}$)**: predicts the next day's return from the previous day's return.\n",
    "- **Ridge Regression (tabular)**: linear model on the frozen feature set from §6. The regularisation parameter $\\alpha$ is chosen using validation Pearson correlation (RMSE used as a tiebreaker).\n",
    "- **XGBoost (tabular)**: tree-based model with early stopping on validation RMSE; we report the best number of trees.\n",
    "\n",
    "Models are trained on the training split and tuned on the validation split, using Pearson correlation as the primary selection metric and RMSE as a secondary tiebreaker. Validation and test results are reported seperately, with no peeking into the test set.\n",
    "\n",
    "> *Why Correlation?* Pearson correlation is scale-free and reflects directional/relative accuracy for daily returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline predictions\n",
    "\n",
    "# Persistence uses the next-day-aligned target series; at time t ir predicts r_t for r_{t+1}\n",
    "persistence_pred_val_series = predict_persistence(y_val)\n",
    "persistence_pred_test_series = predict_persistence(y_test)\n",
    "\n",
    "# Ridge: fit on Train, select alpha on validation correlation; refit on Train+Val for test\n",
    "ridge_model, ridge_diagnostics, ridge_val_pred_series = train_ridge_with_val(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    alphas=tuple(np.logspace(-4, 4, 25)),\n",
    "    refit_on_train_val=True\n",
    ")\n",
    "ridge_test_pred_series = predict_series(ridge_model, X_test, name=\"ridge_pred_test\")\n",
    "\n",
    "# XGBoost: early stopping on validation; refit on train+val (best_n) for test\n",
    "xgb_model, xgb_diagnostics, xgb_val_pred_series = train_xgb_with_val(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=1.0,\n",
    "    early_stopping_rounds=50,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    refit_on_train_val=True,\n",
    ")\n",
    "xgb_test_pred_series = predict_series(xgb_model, X_test, name=\"xgb_pred_test\")\n",
    "\n",
    "#TODO: MOVE ALL FUNCTIONS TO MODULESA\n",
    "# Scoring helpers\n",
    "def score_prediction(true_returns_series: pd.Series, predicted_returns_series:pd.Series):\n",
    "    \"\"\"    Score predictions using the project's alignment/cleaning: returns (pearson_r, rmse, directional_accuracy_percent).\"\"\"\n",
    "    # Wrap predictions as a single-column DataFrame\n",
    "    predicted_df = pd.DataFrame({\"predicted\": pd.Series(predicted_returns_series)})\n",
    "\n",
    "    # Align and drop NaNs (post-lag edges, infs handled upstream)\n",
    "    X_aligned, y_aligned = align_X_y(predicted_df, pd.Series(true_returns_series), dropna=True)\n",
    "\n",
    "    predicted_aligned_series = X_aligned[\"predicted\"].astype(\"float64\")\n",
    "    y_aligned_series = y_aligned.astype(\"float64\")\n",
    "\n",
    "    correlation = pearson_corr(y_aligned_series, predicted_aligned_series)\n",
    "    rmse_value = float(np.sqrt(mean_squared_error(y_aligned_series, predicted_aligned_series)))\n",
    "    directional_accuracy = float((np.sign(y_aligned_series) == np.sign(predicted_aligned_series)).mean() * 100.0)\n",
    "\n",
    "    return correlation, rmse_value, directional_accuracy\n",
    "\n",
    "# Assemble metrics table\n",
    "metrics_rows = [\n",
    "    [\"Persistence\", \n",
    "     *score_prediction(y_val, persistence_pred_val_series),\n",
    "     *score_prediction(y_test, persistence_pred_test_series)\n",
    "    ],\n",
    "    [f\"Ridge (alpha={ridge_diagnostics['best_alpha']:.3g})\", \n",
    "     *score_prediction(y_val, ridge_val_pred_series),\n",
    "     *score_prediction(y_test, ridge_test_pred_series)\n",
    "    ],\n",
    "    [f\"XGBoost (n={xgb_diagnostics['best_n_estimators']})\", \n",
    "     *score_prediction(y_val, xgb_val_pred_series),\n",
    "     *score_prediction(y_test, xgb_test_pred_series)\n",
    "    ]\n",
    "]\n",
    "\n",
    "baseline_metrics_df = pd.DataFrame(\n",
    "    metrics_rows,\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Val Pearson corr\", \"Val RMSE\", \"Val Directional Accuracy %\",\n",
    "        \"Test Pearson corr\", \"Test RMSE\", \"Test Directional Accuracy %\"\n",
    "    ]\n",
    ").set_index(\"Model\")\n",
    "\n",
    "formatters = {\n",
    "    \"Val Pearson corr\": \"{:.3f}\".format,\n",
    "    \"Test Pearson corr\": \"{:.3f}\".format,\n",
    "    \"Val RMSE\": \"{:.3f}\".format,\n",
    "    \"Test RMSE\": \"{:.3f}\".format,\n",
    "    \"Val Directional Accuracy %\": \"{:.2f}\".format,\n",
    "    \"Test Directional Accuracy %\": \"{:.2f}\".format,\n",
    "}\n",
    "print(\"§7 Baseline Metrics\")\n",
    "print(baseline_metrics_df.to_string(formatters=formatters))\n",
    "\n",
    "# inspect chosen hyperparameters at a glance\n",
    "ridge_diagnostics, xgb_diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Targets from close prices\n",
    "targets = make_targets(df[\"close\"], vol_span=20) # -> columns: log_return, ewma_vol_20, y_raw, y_vol\n",
    "\n",
    "# 2) Feature matrix (lag-safe; applies a central lag so X_t predicts y_{t+1})\n",
    "feature_matrix = build_feature_matrix(df, lag_days=1, drop_na_rows=True)\n",
    "\n",
    "# 3) Validations (index, NaNs/inf, basic anti-leakage structure)\n",
    "run_all_validations(\n",
    "    raw=df,\n",
    "    X=feature_matrix,\n",
    "    y=targets[\"y_raw\"].reindex(feature_matrix.index), # align to X\n",
    "    required_raw_cols=(\"open\",\"high\",\"low\",\"close\"),\n",
    "    expected_central_lag_days=1\n",
    ")\n",
    "\n",
    "# 4) Keep a working frame for quick inspection\n",
    "features = feature_matrix.join(targets[[\"y_raw\", \"y_vol\"]], how=\"inner\")\n",
    "print(features.head(10))\n",
    "print(f\"\\nFeature shape: {feature_matrix.shape}, with {feature_matrix.columns.nunique()} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf2167",
   "metadata": {},
   "source": [
    "## 8. LSTM Data Preperation & Architecture\n",
    "\n",
    "We need to turn the frozen feature set into sequences an LSTM can learn from. We'll keep things simple: use the selected colummns, split by time, standardise from the train slice only, window into '(samples, timesteps, features'), and then define a compact LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frozen feature list (fallback to all features if not found)\n",
    "FEATURE_SET_PATH = Path(\"artifacts/feature_set.json\")\n",
    "if FEATURE_SET_PATH.exists():\n",
    "    SELECTED_FEATURES = json.loads(FEATURE_SET_PATH.read_text())[\"selected_features\"]\n",
    "else:\n",
    "    SELECTED_FEATURES = feature_matrix.columns.tolist()\n",
    "    print(\"Frozen feature set not found; using all features.\")\n",
    "\n",
    "# Keep selected numeric features\n",
    "cols = [col for col in SELECTED_FEATURES if col in feature_matrix.columns]\n",
    "X_full = feature_matrix[cols].select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Target: next-day log return aligned to X\n",
    "y_full = targets[\"y_raw\"].reindex(X_full.index).astype(\"float64\")\n",
    "\n",
    "# Align and drop NaNs (post-lag edges, infs handled upstream)\n",
    "X_full, y_full = align_X_y(X_full, y_full, dropna=True)\n",
    "\n",
    "# Subset the dataframes to only include the selected features\n",
    "X_train_lstm = X_train[SELECTED_FEATURES]\n",
    "X_val_lstm = X_val[SELECTED_FEATURES]\n",
    "X_test_lstm = X_test[SELECTED_FEATURES]\n",
    "\n",
    "# The target series (y_train, y_val, y_test) are already defined and aligned from §6\n",
    "print(f\"\\nUsing {len(SELECTED_FEATURES)} features for the LSTM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b01a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standard Scaler to data\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "# Fit scalers on the training data only\n",
    "x_scaler.fit(X_train_lstm)\n",
    "y_scaler.fit(y_train.to_frame())\n",
    "\n",
    "# Transform all splits\n",
    "X_train_scaler = x_scaler.transform(X_train_lstm)\n",
    "X_val_scaler = x_scaler.transform(X_val_lstm)\n",
    "X_test_scaler = x_scaler.transform(X_test_lstm)\n",
    "\n",
    "y_train_scaler = y_scaler.transform(y_train.to_frame())\n",
    "y_val_scaler = y_scaler.transform(y_val.to_frame())\n",
    "y_test_scaler = y_scaler.transform(y_test.to_frame())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d73b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60 \n",
    "\n",
    "def make_sequence(X2d: np.ndarray, y2d: np.ndarray, sequence_length: int):\n",
    "    \"\"\"Transforms time series data into overlapping sequences for LSTM input.\"\"\"\n",
    "    n = len(X2d)\n",
    "    if n <= sequence_length:\n",
    "        raise ValueError(f\"Not enough rows ({n}) for sequence length {sequence_length}.\")\n",
    "    Xs = np.stack([X2d[i:i+sequence_length, :] for i in range(n - sequence_length)])\n",
    "    ys = np.stack([y2d[i+sequence_length-1, :] for i in range(n - sequence_length)])\n",
    "    return Xs.astype(\"float32\"), ys.astype(\"float32\")\n",
    "\n",
    "# Create the final sequenced arrays for the LSTM model\n",
    "X_train_seq, y_train_seq = make_sequence(X_train_scaler, y_train_scaler, sequence_length)\n",
    "X_val_seq, y_val_seq = make_sequence(X_val_scaler, y_val_scaler, sequence_length)\n",
    "X_test_seq, y_test_seq = make_sequence(X_test_scaler, y_test_scaler, sequence_length)\n",
    "\n",
    "print(\"\\n--- Final Data Shapes for LSTM ---\")\n",
    "print(f\"Shape of training data: X={X_train_seq.shape}, y={y_train_seq.shape}\")\n",
    "print(f\"Shape of validation data: X={X_val_seq.shape}, y={y_val_seq.shape}\")\n",
    "print(f\"Shape of test data: X={X_test_seq.shape}, y={y_test_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f07744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "def build_lstm_model(input_shape, learning_rate=1e-3, dropout=0.20):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.LSTM(96, return_sequences=True),\n",
    "        keras.layers.Dropout(dropout),\n",
    "        keras.layers.LSTM(48, return_sequences=False),\n",
    "        keras.layers.Dropout(dropout),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\")]    \n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_lstm_model(input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), learning_rate=1e-3, dropout=0.20)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5d940",
   "metadata": {},
   "source": [
    "That gives us windowed, scaled sequences and an LSTM tuned by validation correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ba50d",
   "metadata": {},
   "source": [
    "## 9.Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf59b1",
   "metadata": {},
   "source": [
    "We optimise with Adam (using a small learning rate for stability) and train for a generous epoch budget while earlystopping on validation Pearson correlation. Pearson is chosen over MSE as it is scale-invariant and better aligned with ranking/forecast objectives.\n",
    "\n",
    "During training we:\n",
    "1. compute val_pearson_corr at the end of each epoch\n",
    "2. early-stop when it plateaus, restoring the best weights, and\n",
    "3. record the best epoch (the one with highest validation correlation)\n",
    "\n",
    "After that model selection step, we refit from scratch on Train+Val for exactly best_epoch + 1 epochs, so the final model uses all in-sample data before evaluating on Test. We keep shuffle=False throughout to preserve temporal ordering.\n",
    "\n",
    "> Notes: \n",
    "> - Pearson is \"higher is better\", so we set mode='max'.\n",
    "> - We log val_pearson_corr into Keras History.history so it shows in curves.\n",
    "> - Artefacts and final model weights are archived for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr_pandas(y_true, y_pred) -> float:\n",
    "    y_true_series = y_true if isinstance(y_true, pd.Series) else pd.Series(np.ravel(y_true))\n",
    "    y_pred_series = pd.Series(np.ravel(y_pred), index=y_true_series.index)\n",
    "    corr = pearson_corr(y_true_series, y_pred_series, raise_on_degenerate=False)\n",
    "    return float(corr) if np.isfinite(corr) else 0.0\n",
    "\n",
    "# 1. Define the custom metric using TensorFlow/TensorFlow Probability\n",
    "def pearson_correlation_metric(y_true, y_pred):\n",
    "    \"\"\"Pearson correlation metric for TensorFlow tensors.\"\"\"\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "    corr = tfp.stats.correlation(y_true, y_pred, sample_axis=0, event_axis=None)\n",
    "    # Return 0.0 if correlation is NaN (e.g., due to zero variance)\n",
    "    return tf.where(tf.math.is_nan(corr), 0.0, corr)\n",
    "\n",
    "# 2. Update the build_lstm_model function to compile the new metric\n",
    "def build_lstm_model(input_shape, learning_rate=1e-3, dropout=0.20):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.LSTM(96, return_sequences=True),\n",
    "        keras.layers.Dropout(dropout),\n",
    "        keras.layers.LSTM(48, return_sequences=False),\n",
    "        keras.layers.Dropout(dropout),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "        loss=\"mse\",\n",
    "        metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\"), pearson_correlation_metric]    \n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Re-build the model so it includes the new compiled metric\n",
    "model = build_lstm_model(input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), learning_rate=1e-3, dropout=0.20)\n",
    "\n",
    "# Run directory\n",
    "run_stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "ARTEFACTS_DIR = f\"artefacts/run-{run_stamp}/\"\n",
    "os.makedirs(ARTEFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# 3. Update the standard callbacks to monitor the new metric\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_pearson_correlation_metric\",\n",
    "    mode=\"max\",\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_on_plateau = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_pearson_correlation_metric\",\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 4. Fit the model using the standard callbacks and sequenced data\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stopping, reduce_on_plateau],\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get the best epoch and correlation from the training history\n",
    "best_epoch = np.argmax(history.history.get(\"val_pearson_correlation_metric\", [0]))\n",
    "best_val_corr = history.history[\"val_pearson_correlation_metric\"][best_epoch]\n",
    "print(f\"\\\\n[Selection] Best epoch: {best_epoch} | Best val Pearson: {best_val_corr:.6f}\")\n",
    "\n",
    "# Save training history + selection\n",
    "pd.DataFrame(history.history).to_parquet(f\"{ARTEFACTS_DIR}training_history.parquet\")\n",
    "with open(f\"{ARTEFACTS_DIR}selection.json\", \"w\") as f:\n",
    "    json.dump({\"best_epoch\": int(best_epoch), \"best_val_pearson\": best_val_corr}, f, indent=2)\n",
    "\n",
    "# 5. Refit final model on combined SEQUENCED Train+Val data\n",
    "X_train_val_seq = np.concatenate([X_train_seq, X_val_seq], axis=0)\n",
    "y_train_val_seq = np.concatenate([y_train_seq, y_val_seq], axis=0)\n",
    "\n",
    "final_model = build_lstm_model(input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), learning_rate=1e-3, dropout=0.20)\n",
    "\n",
    "final_history = final_model.fit(\n",
    "    X_train_val_seq, y_train_val_seq,\n",
    "    epochs=(best_epoch + 1) if best_epoch is not None else 1,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\\\n[Final Fit] Trained on Train+Val for {(best_epoch + 1) if best_epoch is not None else 1} epoch(s). Ready for test scoring.\")\n",
    "\n",
    "pd.DataFrame(final_history.history).to_parquet(f\"{ARTEFACTS_DIR}final_fit_history.parquet\")\n",
    "final_model.save(f\"{ARTEFACTS_DIR}final_model.keras\")\n",
    "\n",
    "# 6. Make Test Predictions using SEQUENCED test data\n",
    "y_test_pred_scaled = final_model.predict(X_test_seq, verbose=0).reshape(-1)\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "test_corr = pearson_corr_pandas(y_test_seq.flatten(), y_test_pred)\n",
    "print(f\"\\\\n[Test] Pearson correlation: {test_corr:.6f}\")\n",
    "\n",
    "pd.DataFrame({\"y_true\": np.ravel(y_test_seq), \"y_pred\": y_test_pred}).to_parquet(f\"{ARTEFACTS_DIR}test_predictions.parquet\")\n",
    "with open(f\"{ARTEFACTS_DIR}test_results.json\", \"w\") as f:\n",
    "    json.dump({\"pearson_corr\": test_corr}, f, indent=2)\n",
    "\n",
    "# Make Validation predictions for diagnostics (from initial early-stopped model)\n",
    "val_pred_scaled = model.predict(X_val_seq, verbose=0).reshape(-1)\n",
    "val_pred_best = y_scaler.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()\n",
    "pd.DataFrame({\"y_true\": np.ravel(y_val_seq), \"y_pred\": val_pred_best}).to_parquet(f\"{ARTEFACTS_DIR}val_results.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8c83f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e296881",
   "metadata": {},
   "source": [
    "## 10. Test Evaluation & Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3564cc0",
   "metadata": {},
   "source": [
    "Once the model is trained and predictions are available for the test set, we map predicted signals to positions and evaluate their performance relative to several baselines and a label-shuffle control. The evaluation quantifies both return and risk characteristics of the strategies.\n",
    "\n",
    "For each model and control:\n",
    "1. **Backtest**: convert predictions into positions and compute net returns using a single-asset backtest framework with realistic trading parameters, including transaction costs, turnover limits, and optional volatility targeting.\n",
    "2. **Wealth Curve**: generate a $1,000 wealth curve for the strategy and compare it to a Buy 7 Hold benchmark.\n",
    "3. **Metrics**: compute key performance statistics including Sharpe ratio, maximum drawdown (MaxDD), turnover, and other relevant summaries. Additional analyses include trade-level summaries, long/short attribution, yearly returns, and regime-based performance by realised volatility.\n",
    "4. **Controls**: Include baselines such as persistence, Ridge, XGBoost predictions, as well as a label-shuffle control for sanity check.\n",
    "\n",
    "All results are aggregated into concise tables and wealth charts, and artefacts (summary statistics, wealth series, and auxiliary tables) are persisted for reproducability. This setup provides a rigorous framework to assess predictive signal quality, trading robustness, and economic significance of model outputs on out-of-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c002e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correct index for the test data after sequencing.\n",
    "# The index starts after the first 'sequence_length - 1' observations are used for the first sequence.\n",
    "test_idx = X_test.index[sequence_length - 1:]\n",
    "\n",
    "# Inverse transform the predictions and true values to get their original log-return scale.\n",
    "y_test_pred_unscaled = y_scaler.inverse_transform(y_test_pred.reshape(-1, 1)).flatten()\n",
    "y_test_true_unscaled = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create correctly indexed pandas Series.\n",
    "pred_lstm_test = pd.Series(y_test_pred_unscaled, index=test_idx, name=\"lstm_pred_test\")\n",
    "y_test_series_log = pd.Series(y_test_true_unscaled, index=test_idx, name=\"y_test_log\")\n",
    "\n",
    "# Align all baseline predictions to the same index as the LSTM for a fair comparison.\n",
    "persistence_pred_test_series = persistence_pred_test_series.reindex(test_idx)\n",
    "ridge_test_pred_series = ridge_test_pred_series.reindex(test_idx)\n",
    "xgb_test_pred_series = xgb_test_pred_series.reindex(test_idx)\n",
    "\n",
    "# Helper to run a backtest and summarise\n",
    "def backtest_and_summarize(scores: pd.Series, y_next_log: pd.Series, name: str):\n",
    "    \"\"\"Runs backtest, generates wealth curve, and summarises performance.\"\"\"\n",
    "    \n",
    "    # Convert log returns to simple returns for accurate PnL compounding in the backtest.\n",
    "    y_next_simple = np.exp(y_next_log) - 1\n",
    "    \n",
    "    backtest = single_asset_backtest(\n",
    "        scores=scores,\n",
    "        y_raw_next=y_next_simple, # Pass simple returns here\n",
    "        z_window=60,\n",
    "        raw_weight_cap=1.0,\n",
    "        target_vol_annual=0.10,\n",
    "        vol_span=20,\n",
    "        cost_bps=5.0,\n",
    "        gross_limit=3.0,\n",
    "        execution=\"close_to_close\",\n",
    "        drop_warmup=True,\n",
    "        use_vol_targeting=True,\n",
    "    )\n",
    "    \n",
    "    # Align backtest results with target series to ensure consistent length\n",
    "    X_aligned, y_aligned_log = align_X_y(backtest[[\"net_return\"]], y_next_log, dropna=False)\n",
    "    backtest_aligned = backtest.loc[X_aligned.index]\n",
    "    \n",
    "    # Wealth (strategy) is calculated from the simple net returns of the backtest\n",
    "    wealth_strat = wealth_curve(\n",
    "        returns=backtest_aligned[\"net_return\"],\n",
    "        starting_wealth=1_000.0,\n",
    "        returns_are_log=False\n",
    "    ).rename(name)\n",
    "    \n",
    "    # Backtest summary\n",
    "    summary = summarize_backtest(\n",
    "        backtest_frame=backtest_aligned,\n",
    "        return_column=\"net_return\",\n",
    "        turnover_column=\"turnover\",\n",
    "        starting_wealth=1_000.0,\n",
    "        returns_are_log=False,\n",
    "        label=name,\n",
    "        fold=\"test\"\n",
    "    )\n",
    "    \n",
    "    # Note: The wealth_bh (Buy&Hold) is now calculated outside this helper for robustness.\n",
    "    return {\"name\": name, \"wealth_strat\": wealth_strat, \"summary\": summary}\n",
    "\n",
    "# Run for LSTM + baselines + label-shuffle control \n",
    "results = []\n",
    "results.append(backtest_and_summarize(pred_lstm_test, y_test_series_log, \"LSTM\"))\n",
    "results.append(backtest_and_summarize(persistence_pred_test_series, y_test_series_log, \"Persistence\"))\n",
    "results.append(backtest_and_summarize(ridge_test_pred_series, y_test_series_log, \"Ridge\"))\n",
    "results.append(backtest_and_summarize(xgb_test_pred_series, y_test_series_log, \"XGBoost\"))\n",
    "\n",
    "# Label-shuffle control (sanity check)\n",
    "rng = np.random.default_rng(42)\n",
    "scores_shuffle = pd.Series(rng.permutation(pred_lstm_test.values), index=pred_lstm_test.index, name=\"Label-Shuffle\")\n",
    "results.append(backtest_and_summarize(scores_shuffle, y_test_series_log, \"Label-Shuffle\"))\n",
    "\n",
    "# Aggregate wealth curves for the plot/table \n",
    "wealth_df = pd.concat([result[\"wealth_strat\"] for result in results], axis=1)\n",
    "\n",
    "# Robustly calculate the Buy & Hold wealth curve from the original `targets` DataFrame\n",
    "buy_hold_returns = targets['y_raw'].reindex(wealth_df.index).dropna()\n",
    "wealth_bh = wealth_curve(\n",
    "    returns=buy_hold_returns,\n",
    "    starting_wealth=1_000.0,\n",
    "    returns_are_log=True  # We know 'y_raw' contains log returns\n",
    ").rename(\"Buy&Hold\")\n",
    "\n",
    "# Combine all wealth curves for plotting\n",
    "wealth_df = pd.concat([wealth_df, wealth_bh], axis=1)\n",
    "\n",
    "# Build concise stats table\n",
    "summary_table = pd.concat([result[\"summary\"].rename(result[\"name\"]) for result in results]).T\n",
    "\n",
    "# Compute Buy & Hold metrics\n",
    "bh_return_series = targets['y_raw'].reindex(wealth_df.index).dropna()\n",
    "bh_row = pd.Series({\n",
    "    \"cagr\": float(annualized_return(bh_return_series, returns_are_log=True)),\n",
    "    \"vol\": float(annualized_volatility(bh_return_series)),\n",
    "    \"sharpe\": float(sharpe_ratio(bh_return_series, risk_free_rate_annual=0.0)),\n",
    "    \"max_drawdown\": float(max_drawdown(wealth_bh)[0]),\n",
    "    \"hit_rate\": float(hit_rate(bh_return_series)),\n",
    "    \"turnover_mean\": np.nan,\n",
    "    \"label\": \"Buy&Hold\",\n",
    "    \"fold\": \"test\"\n",
    "}, name=\"Buy&Hold\")\n",
    "\n",
    "# Join columns then append Buy & Hold\n",
    "stats_df = pd.concat([summary_table, bh_row.to_frame().T], axis=0)\n",
    "\n",
    "# Display the stats table\n",
    "print(\"Backtest & Test Evaluation — key stats (test window)\")\n",
    "table = stats_df.rename(columns={\n",
    "    \"cagr\": \"CAGR\", \"vol\": \"Vol\", \"sharpe\": \"Sharpe\",\n",
    "    \"max_drawdown\": \"MaxDD\", \"turnover_mean\": \"Turnover (mean)\", \"hit_rate\": \"HitRate\"\n",
    "    }).style.format({\n",
    "    \"CAGR\": \"{:.2%}\", \"Vol\": \"{:.2%}\", \"Sharpe\": \"{:.2f}\",\n",
    "    \"MaxDD\": \"{:.2%}\", \"Turnover (mean)\": \"{:.4f}\", \"HitRate\": \"{:.2%}\"\n",
    "}).set_caption(\"Backtest Summary\")\n",
    "\n",
    "table\n",
    "\n",
    "# $1k Wealth Chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "wealth_df.plot(ax=plt.gca())\n",
    "plt.title(\"$1k Wealth — Test Window\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Wealth ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0e1d4",
   "metadata": {},
   "source": [
    "## 11. Conclusions & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b8026e",
   "metadata": {},
   "source": [
    "## 10. Finale: Findings, Limitations & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a26c9",
   "metadata": {},
   "source": [
    "### 10.1 Summary of What We Built\n",
    "This project assembled a **time‑series forecasting pipeline** for SPY daily data:\n",
    "- Retrieved and cleaned OHLCV via Alpaca; engineered **price/ratio/volatility**, **technical indicators** (SMA/EMA, RSI, MACD, Bollinger), plus **statistical** features (Shannon entropy, Hurst exponent, ACF).\n",
    "- Performed **time‑aware feature selection** with a Random Forest to identify the most informative signals for next‑day returns.\n",
    "- Prepared **chronological train/val/test splits**, fit scalers on **train only** to avoid leakage, and created **fixed‑length sequences** for the LSTM.\n",
    "- Trained a **2-layer LSTM** with dropout and early stopping; evaluated both **return prediction metrics** and **price diagnostics** (anchored and chained paths), plus **directional accuracy**, **residuals**, and **stability**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134295c3",
   "metadata": {},
   "source": [
    "### 10.2 Key Takeaways (Interpretation)\n",
    "- **Return predictions** show a very small R² (~0.0075) — expected in a near-efficient market — but a **directional accuracy of ~60%** suggests the model captures some signal in predicting the sign of returns, even if magnitude predictions are noisy.  \n",
    "- **Anchored price path** reconstruction yields a high R² (~0.957) and low MAPE (~0.52%), but this is partly an artefact of using the last known actual price each step — it’s optimistic and less representative of real deployment.  \n",
    "- **Chained price path**, which compounds predicted returns without resetting to actuals, shows more realistic performance: R² drops to ~0.644 and errors are larger (MAE ~10.29, RMSE ~12.35). This better reflects the model’s live trading performance, where prediction errors accumulate.  \n",
    "- **Magnitude vs. direction**: While predicting exact next-day returns remains challenging (high MAPE in returns), the directional edge could be exploitable in a strategy with good risk management.  \n",
    "\n",
    "Overall, the model appears better suited for **trend direction signalling** rather than precise return magnitude forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c13b4",
   "metadata": {},
   "source": [
    "### 10.3 Limitations & Risks\n",
    "- **Non‑stationarity / regime shifts**: Relationships change across time (macro cycles, volatility regimes). A static model can decay quickly without **walk‑forward re‑training**.\n",
    "- **Label definition**: Next‑day return may be too noisy; alternative horizons (e.g., 5‑day) or targets (e.g., **directional** labels) could be more learnable.\n",
    "- **Evaluation bias**: Anchored price diagnostics can look optimistic; **chained** paths are more realistic and should be emphasised.\n",
    "- **Data coverage**: Using only SPY daily bars leaves **news/macro/term structure** unexplored; edge likely improves with richer context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71299d1",
   "metadata": {},
   "source": [
    "### 10.4 Next Steps (High‑Impact First)\n",
    "1. **Walk‑Forward Validation**  \n",
    "   Switch to rolling-window training & evaluation (expand/slide) to mimic live deployment. Log metrics over each window.\n",
    "2. **Hyperparameter Search**  \n",
    "   Tune sequence length, hidden sizes, dropout, learning rate, and batch size (e.g., KerasTuner or Optuna). Start LR at 1e‑3/1e‑4.\n",
    "3. **Targets & Horizons**  \n",
    "   Try directional targets (`sign(return)`) and multi‑horizon returns (e.g., 1/3/5 day). Compare hit rate, precision/recall, and calibration.\n",
    "4. **Features**  \n",
    "   Add **trend/vol regime flags**, **market breadth**, **term structure** (VIX term, yield curve), **seasonality dummies**, and **macro surprise** proxies. Re‑run selection.\n",
    "5. **Baselines & Ensembling**  \n",
    "   Add simple baselines (zero‑return, yesterday’s sign, SMA crossover), linear/elastic‑net, XGBoost/LightGBM, and a TCN. Consider simple **averaging or stacking**.\n",
    "6. **Robustness & Stability**  \n",
    "   Bootstrap test periods, compute **confidence intervals** for metrics, and add **drift detection** (e.g., PSI) to know when to retrain.\n",
    "7. **Reproducibility & Ops**  \n",
    "   Seed everything (NumPy/TF/Python), log versions, and save the **scalers + model** artifacts. Add a small **inference notebook**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
